# -*- coding: utf-8 -*-
"""ML_pen.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16E3mydFPK_bOZguK6ctNiJi9T5UEvZka
"""

!pip install --upgrade gspread
from google.colab import drive
drive.mount('/content/gdrive')

from google.colab import auth
auth.authenticate_user()

!ls

import gspread 
#from oauth2client.client import GoogleCredentials
#gc = gspread.authorize(GoogleCredentials.get_application_default())
from google.auth import default
creds, _ = default()
gc = gspread.authorize(creds)

# workbranch = gc.create('/content/gdrive/MyDrive/weather')
# saved as 'content gdrive MyDrive weather.gsheet'
# path: /content/gdrive/MyDrive/ content gdrive MyDrive weather.gsheet
!cd /content/gdrive/MyDrive 
#already at MyDrive
#workbranch = gc.create('weather')

!ls #??

import pandas as pd
import numpy as np

# /content/gdrive/MyDrive/test.csv 
#df = pd.read_csv('test.csv')
df = pd.read_csv('/content/gdrive/MyDrive/test.csv')
print(df)
data = np.array(df, dtype = float)
print(data)

DATA = []
for i in range(0, 137, 6):
#  df_micro1.append(df.aloc(i))
  DATA.append(data[:,i:i+6])

print(np.shape(DATA))
N = np.shape(DATA)[0]
t = np.shape(DATA)[1]
n = np.shape(DATA)[2]
print(DATA)

micro1 = []
for i in range(0, 137, 6):
#  df_micro1.append(df.aloc(i))
  micro1.append(data[:,i])

print(np.shape(micro1))
print(micro1)

intensity1 = []
for i in range(1, 137, 6):
  intensity1.append(data[:,i])
print(np.shape(intensity1))
print(intensity1)

micro2 = []
for i in range(2, 137, 6):
  micro2.append(data[:,i])
print(np.shape(micro2))
print(micro2)

intensity2 = []
for i in range(2, 137, 6):
  intensity2.append(data[:,i])
print(np.shape(intensity2))
print(intensity2)

import matplotlib.pyplot as plt
from scipy.fftpack import fft
p1 = plt.figure()
plt.plot(intensity1)
intensity1_Fq = fft(intensity1)
p2 = plt.figure()
plt.plot(intensity1_Fq)

intensity1[1]
np.shape(intensity1)

p1_2 = plt.figure()
plt.plot(intensity2)
intensity2_Fq = fft(intensity2)
p2_2 = plt.figure()
plt.plot(intensity2_Fq)

I1 = np.array(intensity1,dtype = float)
print(np.shape(I1))
N = np.shape(I1)[0]
t = np.shape(I1)[1]
print(np.shape(I1[0]))
print(np.shape(I1[1][:]))
I2 = np.array(intensity2,dtype = float)

P1 = np.array(micro1,dtype = float)
P2 = np.array(micro2,dtype = float)

import numpy as np
import matplotlib.pyplot as plt
from sklearn.feature_extraction import image
from sklearn.cluster import spectral_clustering

img1 = np.zeros((t,2))
img2 = np.zeros((t,2))
for i in range(N):
  img1 += np.array([I1[i],I2[i]],dtype = float).reshape(t,2)
  img2 += np.array([P1[i],P2[i]],dtype = float).reshape(t,2)
print(np.shape(img1))
print(type(img1))
mask1 = img1.astype(bool)
img1 = img1.astype(float)
img1 += 1 + 0.2 * np.random.randn(*img1.shape)
mask2 = img2.astype(bool)
img2 = img2.astype(float)
img2 += 1 + 0.2 * np.random.randn(*img2.shape)

graph1 = image.img_to_graph(img1, mask=mask1)
graph1.data = np.exp(-graph1.data/graph1.data.std())
graph2 = image.img_to_graph(img2, mask=mask1)
graph2.data = np.exp(-graph2.data/graph1.data.std())

labels1 = spectral_clustering(graph1, n_clusters=N, eigen_solver='arpack')
label1_im = -np.ones(mask1.shape)
label1_im[mask1] = labels1
plt.matshow(img1)
plt.matshow(label1_im)

labels2 = spectral_clustering(graph1, n_clusters=N, eigen_solver='arpack')
label2_im = -np.ones(mask2.shape)
label2_im[mask2] = labels2
plt.matshow(img2)
plt.matshow(label2_im)



from sklearn import metrics
from matplotlib.colors import ListedColormap
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# (23, 79, 6) N t n 
X = np.array(DATA, dtype = float).reshape(N*t, n)
y = (np.repeat(np.linspace(0,N-1,N),t)).reshape(N*t, 1)
print(np.shape(X))
print(np.shape(y))
print(X)
print(y)

from sklearn.tree import DecisionTreeClassifier
#self train
Res1 = []
Acc = []
Report = []
Res2 = []
Acc = []
Report = []
for d in [4,6,8,10,12,14,16,18,22,24]:
  tree = DecisionTreeClassifier(criterion='gini',max_depth=d,random_state=1)
  tree.fit(X,y)
  res1 = tree.predict(X)
  Res1.append(np.reshape(res1,np.shape(y)))
  diff = np.reshape(res1,np.shape(y)) == y
  print(np.shape(diff))
  print(diff)
  acc = sum(diff)/len(diff)
  Acc.append(acc)
  print(res1)
  print(acc)
  print(metrics.classification_report(y, res1, digits=4)) #
  Report.append(metrics.classification_report(y, res1, digits=4))
print(Acc)
plt.plot(Acc)
plt.title('self-study ACC v.s max_depth')

from sklearn.model_selection import KFold
n_splits = 2
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=1,stratify=y)
kf = KFold(n_splits,random_state=1,shuffle=True)
#kf.get_n_splits(X)
train_index, test_index = kf.split(X_train)
print(np.shape(train_index[-1]))
print(np.shape(test_index[-1]))
X_trainKF,X_testKF,y_trainKF,y_testKF = X_train[train_index[-1]],X_train[test_index[-1]],y_train[train_index[-1]],y_train[test_index[-1]]

from sklearn.tree import DecisionTreeClassifier
#train with 0.3 as test 
Res1 = []
Acc = []
Report = []
for d in [4,6,8,10,12,14,16,18,22,24]:
  tree = DecisionTreeClassifier(criterion='gini',max_depth=d,random_state=1)
  tree.fit(X,y)
  res1 = tree.predict(X_test)
  Res1.append(np.reshape(res1,np.shape(y_test)))
  diff = np.reshape(res1,np.shape(y_test)) == y_test
  print(np.shape(diff))
  print(diff)
  acc = sum(diff)/len(diff)
  Acc.append(acc)
  print(res1)
  print(acc)
  print(metrics.classification_report(y_test, res1, digits=4)) #
  Report.append(metrics.classification_report(y_test, res1, digits=4))

print(Acc)
plt.plot(Acc)
plt.title('study on 0.3 test ACC v.s max_depth')

from sklearn.tree import DecisionTreeClassifier
#train with KFold =2 as test 
Res1KF = []
AccKF = []
ReportKF = []
Res2KF = []
Acc2KF = []
Report2KF = []
for d in range(1,24):
  tree = DecisionTreeClassifier(criterion='gini',max_depth=d,random_state=1)
  tree.fit(X_train,y_train)
  res1 = tree.predict(X_trainKF)
  Res1KF.append(np.reshape(res1,np.shape(y_trainKF)))
  diff = np.reshape(res1,np.shape(y_trainKF)) == y_trainKF
  print(np.shape(diff))
  print(diff)
  acc = sum(diff)/len(diff)
  AccKF.append(acc)
  print(res1)
  print(acc)
  print(metrics.classification_report(y_trainKF, res1, digits=4)) #

  res2 = tree.predict(X_testKF)
  Res2KF.append(np.reshape(res2,np.shape(y_testKF)))
  diff2 = np.reshape(res2,np.shape(y_testKF)) == y_testKF
  print(np.shape(diff2))
  print(diff2)
  acc2 = sum(diff2)/len(diff2)
  Acc2KF.append(acc2)
  print(res2)
  print(acc2)
  print(metrics.classification_report(y_testKF, res2, digits=4)) #

print(AccKF)
plt.plot(AccKF)
print(Acc2KF)
plt.plot(Acc2KF)
plt.legend(['test','train'])
plt.title('study on 0.3 KFold=2 test ACC v.s max_depth')

#30%测试数据 70%训练数据 stratify=y表示训练数据和测试数据具有相同的类别比例
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=1,stratify=y)
print(np.shape(X_train))
print(np.shape(y_train))
sc = StandardScaler()      #估算mu和sigma
sc.fit(X_train)            #使用mu和sigma对数据进行标准化
X_train_std = sc.transform(X_train)
X_test_std = sc.transform(X_test)
print(X_train_std)
print(X_test_std)
p1 = plt.figure()
plt.plot(X_train)
p2 = plt.figure()
plt.plot(X_train_std)

from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

#I_y = np.reshape(np.array(range(N),dtype = int),N,1)
I_y = np.reshape(np.repeat(np.array(range(N),dtype = int),79),(N*t,1))
I = np.reshape([np.reshape(I1,(N*t,1)),np.reshape(I2,(N*t,1))],(N*t,2))
I_fq = np.reshape([np.reshape(intensity1_Fq,(N*t,1)),np.reshape(intensity2_Fq,(N*t,1))],(N*t,2))
#print(I_y)
print(np.shape(I1))
print(np.shape(I))
print(np.shape(I_y))
#30%测试数据 70%I训练数据 stratify=y表示训练数据和测试数据具有相同的类别比例
#X_train,X_test,y_train,y_test = train_test_split(I1,I_y,test_size=0.3,random_state=1,stratify=I_y)
X_train,X_test,y_train,y_test = train_test_split(I,I_y,test_size=0.3,random_state=1,stratify=I_y[:])

sc = StandardScaler()      #估算mu和sigma
sc.fit(X_train)            #使用mu和sigma对数据进行标准化
X_train_std = sc.transform(X_train)
X_test_std = sc.transform(X_test)
#print(X_train_std)
#print(X_test_std)
p1 = plt.figure()
plt.plot(X_train)
p2 = plt.figure()
plt.plot(X_train_std)

X_train,X_test,y_train,y_test = train_test_split(abs(I_fq),I_y,test_size=0.3,random_state=1,stratify=I_y[:])
sc = StandardScaler()      #估算mu和sigma
sc.fit(X_train)            #使用mu和sigma对数据进行标准化
X_train_std = sc.transform(X_train)
X_test_std = sc.transform(X_test)
#print(X_train_std)
#print(X_test_std)
p1 = plt.figure()
plt.plot(X_train)
p2 = plt.figure()
plt.plot(X_train_std)

X_train,X_test,y_train,y_test = train_test_split(abs(I_fq),I_y,test_size=0.3,random_state=1,stratify=I_y[:])

from sklearn.model_selection import KFold
n_splits = 2
#X_train,X_test,y_train,y_test = X_train_std,X_test_std,y_train,y_test
kf = KFold(n_splits,random_state=1,shuffle=True)
#kf.get_n_splits(X)
train_index, test_index = kf.split(I)
print(np.shape(train_index[-1]))
print(np.shape(test_index[-1]))
print(train_index[-1])
X_trainKF1,X_testKF1,y_trainKF1,y_testKF1 = I[train_index[-1],0],I[test_index[-1],0],I_y[train_index[-1]],I_y[test_index[-1]]
X_trainKF2,X_testKF2,y_trainKF2,y_testKF2 = abs(I_fq[train_index[-1],1]),abs(I_fq[test_index[-1],1]),I_y[train_index[-1]],I_y[test_index[-1]]

from sklearn.tree import DecisionTreeClassifier
#train with 0.3 as test 
Res1 = []
Acc = []
Report = []
Res2 = []
Acc2 = []
Report2 = []
for d in [4,6,8,10,12,14,16,18,22,24]:
  tree = DecisionTreeClassifier(criterion='gini',max_depth=d,random_state=1)
  tree.fit(X_train,y_train)
  res1 = tree.predict(X_test)
  Res1.append(np.reshape(res1,np.shape(y_test)))
  diff = np.reshape(res1,np.shape(y_test)) == y_test
  print(np.shape(diff))
  print(diff)
  acc = sum(diff)/len(diff)
  Acc.append(acc)
  print(res1)
  print(acc)
  print(metrics.classification_report(y_test, res1, digits=4)) #
  Report.append(metrics.classification_report(y_test, res1, digits=4))

  tree2 = DecisionTreeClassifier(criterion='gini',max_depth=d,random_state=1)
  tree2.fit(X_train_std,y_train)
  res2 = tree.predict(X_test_std)
  Res2.append(np.reshape(res2,np.shape(y_test)))
  diff2 = np.reshape(res2,np.shape(y_test)) == y_test
  print(np.shape(diff2))
  print(diff2)
  acc2 = sum(diff2)/len(diff2)
  Acc2.append(acc2)
  print(res2)
  print(acc2)
  print(metrics.classification_report(y_test, res2, digits=4)) #
  Report.append(metrics.classification_report(y_test, res1, digits=4))

print(Acc)
plt.plot(Acc)
print(Acc2)
plt.plot(Acc2)
plt.legend(['origin', 'after_transform'])

#plt.title('study on 0.3 test intensity ACC v.s max_depth')
plt.title('study on 0.3 test intensity frequency ACC v.s max_depth')

from sklearn.tree import DecisionTreeClassifier
#train with KFold =2 as test 
Res1KF = []
AccKF = []
ReportKF = []
ResKF2 = []
AccKF2 = []
ReportKF2 = []
print(np.shape(X_trainKF1))
print(np.shape(y_trainKF1))
print(np.shape(X_trainKF2))
print(np.shape(y_trainKF2))

for d in range(1,36):
  tree = DecisionTreeClassifier(max_depth=d,random_state=1)
  tree.fit(np.reshape(X_trainKF1,(len(X_trainKF1),1)),y_trainKF1)
  res1 = tree.predict(np.reshape(X_trainKF1,(len(X_trainKF1),1)))
  Res1KF.append(np.reshape(res1,np.shape(y_trainKF1)))
  diff1 = np.reshape(res1,np.shape(y_trainKF1)) == y_trainKF1
  print(np.shape(diff1))
  print(diff1)
  acc1 = sum(diff1)/len(diff1)
  AccKF.append(acc1)
  print(res1)
  print(acc1)
  print(metrics.classification_report(y_trainKF1, res1, digits=4)) #


  tree.fit(np.reshape(X_trainKF2,(len(X_trainKF2),1)),y_trainKF2)
  res2 = tree.predict(np.reshape(X_trainKF2,(len(X_trainKF2),1)))
  ResKF2.append(np.reshape(res2,np.shape(y_trainKF2)))
  diff2 = np.reshape(res2,np.shape(y_trainKF2)) == y_trainKF2
  print(np.shape(diff2))
  print(diff2)
  acc2 = sum(diff2)/len(diff2)
  AccKF2.append(acc2)
  print(res2)
  print(acc2)
  print(metrics.classification_report(y_trainKF2, res2, digits=4)) #

print(AccKF)
plt.plot(AccKF)
print(AccKF2)
plt.plot(AccKF2)
plt.legend(['Intensity 1','Intensity 2'])
#plt.title('study on 0.3 KFold=2 Intensity test ACC v.s max_depth')
plt.title('study on 0.3 KFold=2 intensity frequency ACC v.s max_depth')

from sklearn.tree import DecisionTreeClassifier

# Parameters
n_classes = N
plot_colors = "bry"
plot_step = 0.02
m = n
m = 2

#print([i for i in enumerate(np.unique(y))])


col = []
for i in range(n):
  for j in range(i+1,n):
    col.append([i, j])
    j += 1
print(col)

print(np.shape(X_train))
print(np.shape(y_train))

#for pairidx, pair in enumerate(col):
for pairidx, pair in enumerate([0,1]):
  print(np.array(pair,dtype = int))
  # We only take the two corresponding features
  X = X_train
  y = y_train
  print(np.shape(X))
  # Train
  clf = DecisionTreeClassifier().fit(X, y)
  # Plot the decision boundary
  plt.plot()
  x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
  y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
#x_min, x_max = X.min() - 1, X.max() + 1
#y_min, y_max = y.min() - 1, y.max() + 1
#  x_min, x_max = X[:, pair].min() - 1, X[:, pair].max() + 1
#  y_min, y_max = X[:, pair].min() - 1, X[:, pair].max() + 1
  xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step), np.arange(y_min, y_max, plot_step))
  Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]) 
  Z = Z.reshape(xx.shape)
  cs = plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)
  plt.xlabel(str(pair[0]))
  plt.ylabel(str(pair[1]))
#plt.xlabel(str(0))
#plt.ylabel(str(1))
  plt.axis("tight")
  # Plot the training points（explit in 2D scatter）
  for i, color in zip(range(n_classes), plot_colors):
    idx = np.where(y == i)
    plt.scatter(X[idx, 0], X[idx, 1], c=color, label=str([i]),cmap=plt.cm.Paired)
    plt.axis("tight")
plt.suptitle("Decision surface of a decision tree using paired features")
plt.legend()
plt.show()



#

print(X_train_std)
print(np.shape(X_train_std))
print(X_train_std[:,0])

X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=1,stratify=y)

from sklearn.model_selection import KFold
n_splits = 2
#X_train,X_test,y_train,y_test = X_train_std,X_test_std,y_train,y_test
kf = KFold(n_splits,random_state=1,shuffle=True)
#kf.get_n_splits(X)
train_index, test_index = kf.split(X)
print(np.shape(train_index[-1]))
print(np.shape(test_index[-1]))
print(train_index[-1])
X_trainKF,X_testKF,y_trainKF,y_testKF = X[train_index[-1],:],X[test_index[-1],:],y[train_index[-1]],y[test_index[-1]]

from sklearn.neighbors import KNeighborsClassifier
ACCKNN = []
ACCKNN_KF = []
for nn in range(1,2*N):
  knn = KNeighborsClassifier(n_neighbors=nn,p=2,metric="minkowski")
  knn.fit(X_train,y_train)
  resknn = knn.predict(X_test)
 # print(resknn)
  print(metrics.classification_report(y_test, resknn, digits=4)) 
  acc = sum(resknn.ravel()==y_test.ravel())/len(y_test.ravel())
  ACCKNN.append(acc)

  knn.fit(X_trainKF,y_trainKF)
  resknnKF = knn.predict(X_testKF)
#  print(resknn)
  print(metrics.classification_report(y_testKF, resknnKF, digits=4)) 
  acckf = sum(resknnKF.ravel()==y_testKF.ravel())/len(y_testKF.ravel())
  ACCKNN_KF.append(acckf)

#print(ACCKNN)
print(np.shape(ACCKNN))
plt.plot(ACCKNN)
#print(ACCKNN_KF)
print(np.shape(ACCKNN_KF))
plt.plot(ACCKNN_KF)
plt.legend(['without kfold','after kfold'])
#plt.title('study on 0.3 KFold=2 Intensity test ACC v.s max_depth')
plt.title('KNN study on 0.3 KFold=2 ACC v.s neighbor numbers')

# Parameters
n_classes = N
plot_colors = "bry"
plot_step = 0.02
m = n
m = 2

#print([i for i in enumerate(np.unique(y))])

col = []
for i in range(n):
  for j in range(i+1,n):
    col.append([i, j])
    j += 1
print(col)

print(np.shape(X_train))
print(np.shape(y_train))

for pairidx, pair in enumerate(col):
#for pairidx, pair in enumerate([[0,1][1,2]]):
  print(np.array(pair,dtype = int))
  # We only take the two corresponding features
  X = X_train
  y = y_train
  print(np.shape(X))
  # Train
  clf = knn.fit(X, y)
  # Plot the decision boundary
  plt.plot()
  x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
  y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
#x_min, x_max = X.min() - 1, X.max() + 1
#y_min, y_max = y.min() - 1, y.max() + 1
#  x_min, x_max = X[:, pair].min() - 1, X[:, pair].max() + 1
#  y_min, y_max = X[:, pair].min() - 1, X[:, pair].max() + 1
  xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step), np.arange(y_min, y_max, plot_step))
  Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]) 
  Z = Z.reshape(xx.shape)
  cs = plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)
  plt.xlabel(str(pair[0]))
  plt.ylabel(str(pair[1]))
#plt.xlabel(str(0))
#plt.ylabel(str(1))
  plt.axis("tight")
  # Plot the training points（explit in 2D scatter）
  for i, color in zip(range(n_classes), plot_colors):
    idx = np.where(y == i)
    plt.scatter(X[idx, 0], X[idx, 1], c=color, label=str([i]),cmap=plt.cm.Paired)
    plt.axis("tight")
plt.suptitle("Decision surface of a decision tree using paired features")
plt.legend()
plt.show()

#SVM分类 核函数对非线性分类问题建模(gamma=0.20)
from sklearn.svm import SVC
ACCSVC = []
ACCSVCKF = []
G = np.linspace(0.001, 0.5, 100)
cC = np.linspace(0.001, 100, 100)
#for g in np.linspace(0.001, 0.005, 0.5):
for g in range(100):
  c = cC[i]
  svm = SVC(kernel='rbf',random_state=1,C = float(100*g+0.0001)) #较小的gamma有较松的决策边界, c 在(0,inf) for regulation注意过拟合, defaullt 最优，
  svm.fit(X_train,y_train)
  ressvm = svm.predict(X_test)
#  print(ressvm)
  print(metrics.classification_report(y_test, ressvm, digits=4))
  acc = sum(ressvm.ravel()==y_test.ravel())/len(y_test.ravel())
  ACCSVC.append(acc)

  svm.fit(X_trainKF,y_trainKF)
  ressvm2 = svm.predict(X_testKF)
#  print(ressvm2)
  print(metrics.classification_report(y_testKF, ressvm2, digits=4))
  acc2 = sum(ressvm2.ravel()==y_testKF.ravel())/len(y_testKF.ravel())
  ACCSVCKF.append(acc2)

print(np.shape(ACCSVC))
plt.plot(ACCSVC)
print(np.shape(ACCSVCKF))
plt.plot(ACCSVCKF)
plt.legend(['without kfold','after kfold'])
plt.title('SVC study on 0.3 KFold=2 ACC v.s C')

# Parameters
n_classes = N
plot_colors = "bry"
plot_step = 0.02
m = n
m = 2

#print([i for i in enumerate(np.unique(y))])

col = []
for i in range(n):
  for j in range(i+1,n):
    col.append([i, j])
    j += 1
print(col)

print(np.shape(X_train))
print(np.shape(y_train))

for pairidx, pair in enumerate(col):
#for pairidx, pair in enumerate([[0,1][1,2]]):
  print(np.array(pair,dtype = int))
  # We only take the two corresponding features
  X = X_train
  y = y_train
  print(np.shape(X))
  # Train
  clf = svm.fit(X, y)
  # Plot the decision boundary
  plt.plot()
  x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
  y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
#x_min, x_max = X.min() - 1, X.max() + 1
#y_min, y_max = y.min() - 1, y.max() + 1
#  x_min, x_max = X[:, pair].min() - 1, X[:, pair].max() + 1
#  y_min, y_max = X[:, pair].min() - 1, X[:, pair].max() + 1
  xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step), np.arange(y_min, y_max, plot_step))
  Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]) 
  Z = Z.reshape(xx.shape)
  cs = plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)
  plt.xlabel(str(pair[0]))
  plt.ylabel(str(pair[1]))
#plt.xlabel(str(0))
#plt.ylabel(str(1))
  plt.axis("tight")
  # Plot the training points（explit in 2D scatter）
  for i, color in zip(range(n_classes), plot_colors):
    idx = np.where(y == i)
    plt.scatter(X[idx, 0], X[idx, 1], c=color, label=str([i]),cmap=plt.cm.Paired)
    plt.axis("tight")
plt.suptitle("Decision surface of a decision tree using paired features")
plt.legend()
plt.show()

from sklearn.linear_model import LogisticRegression
ACClr = []
ACClrKF = []
for c in range(100):
  lr = LogisticRegression(C=c*10+0.0001)
  lr.fit(X_train,y_train)
  res4 = lr.predict(X_test)
  print(metrics.classification_report(y_test, res4, digits=4))
  acc = sum(res4.ravel()==y_test.ravel())/len(y_test.ravel())
  ACClr.append(acc)

  lr.fit(X_test,y_test)
  res4KF = lr.predict(X_testKF)
  print(metrics.classification_report(y_testKF, res4KF, digits=4))
  acckf = sum(res4KF.ravel()==y_testKF.ravel())/len(y_testKF.ravel())
  ACClrKF.append(acckf)

print(np.shape(ACClr))
plt.plot(ACClr)
print(np.shape(ACClrKF))
plt.plot(ACClrKF)
plt.legend(['without kfold','after kfold'])
plt.title('Logistic Regression study on 0.3 KFold=2 ACC v.s C')

# Parameters
n_classes = N
plot_colors = "bry"
plot_step = 0.02
m = n
m = 2

#print([i for i in enumerate(np.unique(y))])

col = []
for i in range(n):
  for j in range(i+1,n):
    col.append([i, j])
    j += 1
print(col)

print(np.shape(X_train))
print(np.shape(y_train))

for pairidx, pair in enumerate(col):
#for pairidx, pair in enumerate([[0,1][1,2]]):
  print(np.array(pair,dtype = int))
  # We only take the two corresponding features
  X = X_train
  y = y_train
  print(np.shape(X))
  # Train
  clf = svm.fit(X, y)
  # Plot the decision boundary
  plt.plot()
  x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
  y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
#x_min, x_max = X.min() - 1, X.max() + 1
#y_min, y_max = y.min() - 1, y.max() + 1
#  x_min, x_max = X[:, pair].min() - 1, X[:, pair].max() + 1
#  y_min, y_max = X[:, pair].min() - 1, X[:, pair].max() + 1
  xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step), np.arange(y_min, y_max, plot_step))
  Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]) 
  Z = Z.reshape(xx.shape)
  cs = plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)
  plt.xlabel(str(pair[0]))
  plt.ylabel(str(pair[1]))
#plt.xlabel(str(0))
#plt.ylabel(str(1))
  plt.axis("tight")
  # Plot the training points（explit in 2D scatter）
  for i, color in zip(range(n_classes), plot_colors):
    idx = np.where(y == i)
    plt.scatter(X[idx, 0], X[idx, 1], c=color, label=str([i]),cmap=plt.cm.Paired)
    plt.axis("tight")
plt.suptitle("Decision surface of a decision tree using paired features")
plt.legend()
plt.show()

from scipy.ndimage import convolve
from sklearn import linear_model, datasets, metrics
from sklearn.neural_network import BernoulliRBM
from sklearn.pipeline import Pipeline

def nudge_dataset(X):
    """
    分別做上左右下一像素的平移，製造更多的訓練資料讓模型訓練出來更強健
    """
    direction_vectors = [
        [[0, 1, 0],
         [0, 0, 0],
         [0, 0, 0]],
        [[0, 0, 0],
         [1, 0, 0],
         [0, 0, 0]],
        [[0, 0, 0],
         [0, 0, 1],
         [0, 0, 0]],
        [[0, 0, 0],
         [0, 0, 0],
         [0, 1, 0]]]
    shift = lambda x, w: convolve(x.reshape((8, 8)), mode='constant',weights=w).ravel()
    X = np.concatenate([X] + [np.apply_along_axis(shift, 1, X, vector) for vector in direction_vectors])
#    Y = np.concatenate([Y] for _ in range(5)], axis=0)
    return X


print(np.shape(X))
print(np.shape(y))
Ximg = np.array(np.reshape(np.repeat(X.ravel(),64),(len(X.ravel()),64)), dtype ='float32')
Yimg = np.array(np.reshape(np.repeat((np.repeat(y,6)).ravel(),64),(len((np.repeat(y,6)).ravel()),64)), dtype ='int')
print(np.shape(Ximg))
print(np.shape(Yimg))
XI= nudge_dataset(Ximg)
XI = (XI - np.min(abs(XI), 0)) / (np.max(abs(XI), 0) + 0.0001)  # 將灰階影像降尺度到[0,1]
print(np.shape(XI))
Y = np.reshape(np.repeat(Yimg,5),np.shape(XI))
print(np.shape(Y))

X_train, X_test, Y_train, Y_test = train_test_split(XI,Y,test_size=0.3,random_state=0)

from sklearn.model_selection import KFold
n_splits = 2
#X_train,X_test,y_train,y_test = X_train_std,X_test_std,y_train,y_test
kf = KFold(n_splits,random_state=1,shuffle=True)
#kf.get_n_splits(X)
train_index, test_index = kf.split(X)
print(np.shape(train_index[-1]))
print(np.shape(test_index[-1]))
print(train_index[-1])
X_trainKF,X_testKF,y_trainKF,y_testKF = X[train_index[-1],:],X[test_index[-1],:],y[train_index[-1]],y[test_index[-1]]

print(np.shape(X_train))
print(np.shape(Y_train[:,0]))

# Models we will use
logistic = linear_model.LogisticRegression()
rbm = BernoulliRBM(random_state=0, verbose=True)
classifier = Pipeline(steps=[('rbm', rbm), ('logistic', logistic)])
# 省去cross-validation去比較, 此參數是使用GridSearchCV找出來的. 
#GridSratch 就是將參數設定好，跑過全部參數後去找結果最好的一組參數
#rbm.learning_rate = 0.06
#logistic.n_components = 100 表示隱藏層單元為100，即表示萃取出100個特徵，特徵萃取的越多準確率會越高，但越耗時間
rbm.n_components = 1000
rbm.n_iter = 10
logistic.n_components = 1000
#仍进行寻找 最好的 learning_rate 在 np.linspace(0.001,0.5,100)
ACCRBM = []
ACCRBMKF = []
ACCLR2 = []
ACCLR2KF = []
LR = np.linspace(0.001,0.5,50)
for i in range(50):
  rbm.learning_rate = LR[i]
  classifier.fit(X_train, Y_train[:,0])
#  rbm.fit(X_train, Y_train)
  resrbm = classifier.predict(X_test)
  print(metrics.classification_report(Y_test[:,0].ravel(), resrbm.ravel(), digits=4))
  acc = sum(resrbm.ravel()==Y_test[:,0].ravel())/len(Y_test[:,0].ravel())
  ACCRBM.append(acc)

  classifier.fit(X_trainKF, y_trainKF1[:,0])
#  rbm.fit(X_trainKF, Y_trainKF)
  resrbmkf = classifier.predict(X_testKF)
  print(metrics.classification_report(y_testKF[:,0].ravel(), resrbmkf.ravel(), digits=4))
  acckf = sum(resrbmkf.ravel()==y_testKF[:,0].ravel())/len(y_testKF[:,0].ravel())
  ACCRBMKF.append(acckf)

  logistic_classifier = linear_model.LogisticRegression(C=1000.0*LR[i]+1.0)
  logistic_classifier.fit(X_train, Y_train[:,0])
  reslr2 = logistic_classifier.predict(X_test)
  print(metrics.classification_report(Y_test[:,0], reslr2, digits=4))
  acc2 = sum(reslr2.ravel()==Y_test[:,0].ravel())/len(Y_test[:,0].ravel())
  ACCLR2.append(acc2)

  logistic_classifier.fit(X_trainKF, y_trainKF[:,0])
  reslr2kf = logistic_classifier.predict(X_testKF)
  print(metrics.classification_report(y_testKF[:,0], reslr2kf, digits=4))
  acc2kf = sum(reslr2kf.ravel()==y_testKF[:,0].ravel())/len(y_testKF[:,0].ravel())
  ACCLR2KF.append(acc2kf)

print(np.shape(ACCRBM))
plt.plot(ACCRBM)
print(np.shape(ACCRBMKF))
plt.plot(ACCRBMKF)
print(np.shape(ACCLR2))
plt.plot(ACCLR2)
print(np.shape(ACCLR2KF))
plt.plot(ACCLR2KF)
plt.legend(['RBM without kfold','RBM after kfold','LR2 without kfold','LR2 after kfold'])
plt.title('RBM Logistic Regression study on 0.3 KFold=2 ACC v.s learning rate(1000cells)')

X_train, X_test, Y_train, Y_test = train_test_split(X,y,test_size=0.3,random_state=0)

from sklearn.model_selection import KFold
n_splits = 2
#X_train,X_test,y_train,y_test = X_train_std,X_test_std,y_train,y_test
kf = KFold(n_splits,random_state=1,shuffle=True)
#kf.get_n_splits(X)
train_index, test_index = kf.split(X)
print(np.shape(train_index[-1]))
print(np.shape(test_index[-1]))
print(train_index[-1])
X_trainKF,X_testKF,y_trainKF,y_testKF = X[train_index[-1],:],X[test_index[-1],:],y[train_index[-1]],y[test_index[-1]]

from sklearn.naive_bayes import GaussianNB
ACCNB = []
ACCNBKF = []
gnb = GaussianNB()
gnb.fit(X_train,y_train)
res5 = gnb.predict(X_test)
print(metrics.classification_report(y_test.ravel(), res5.ravel(), digits=4))
acc = sum(res5.ravel()==y_test.ravel())/len(y_test.ravel())
ACCNB.append(acc)

gnb.fit(X_trainKF,y_trainKF)
res5kf = gnb.predict(X_testKF)
print(metrics.classification_report(y_testKF.ravel(), res5kf.ravel(), digits=4))
acckf = sum(res5kf.ravel()==y_testKF.ravel())/len(y_testKF.ravel())
ACCNBKF.append(acckf)

# Parameters
n_classes = N
plot_colors = "bry"
plot_step = 0.02
m = n
m = 2

#print([i for i in enumerate(np.unique(y))])

col = []
for i in range(n):
  for j in range(i+1,n):
    col.append([i, j])
    j += 1
print(col)

print(np.shape(X_train))
print(np.shape(y_train))

for pairidx, pair in enumerate(col):
#for pairidx, pair in enumerate([[0,1][1,2]]):
  print(np.array(pair,dtype = int))
  # We only take the two corresponding features
  X = X_train
  y = y_train
  print(np.shape(X))
  # Train
  clf = gnb.fit(X, y)
  # Plot the decision boundary
  plt.plot()
  x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
  y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
#x_min, x_max = X.min() - 1, X.max() + 1
#y_min, y_max = y.min() - 1, y.max() + 1
#  x_min, x_max = X[:, pair].min() - 1, X[:, pair].max() + 1
#  y_min, y_max = X[:, pair].min() - 1, X[:, pair].max() + 1
  xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step), np.arange(y_min, y_max, plot_step))
  Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]) 
  Z = Z.reshape(xx.shape)
  cs = plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)
  plt.xlabel(str(pair[0]))
  plt.ylabel(str(pair[1]))
#plt.xlabel(str(0))
#plt.ylabel(str(1))
  plt.axis("tight")
  # Plot the training points（explit in 2D scatter）
  for i, color in zip(range(n_classes), plot_colors):
    idx = np.where(y == i)
    plt.scatter(X[idx, 0], X[idx, 1], c=color, label=str([i]),cmap=plt.cm.Paired)
    plt.axis("tight")
plt.suptitle("Decision surface of a decision tree using paired features")
plt.legend()
plt.show()

from sklearn.ensemble import RandomForestClassifier
ACCRF = []
ACCRFKF = [] 
for nf in range(1,2*N):
  forest = RandomForestClassifier(criterion='gini',n_estimators=nf,random_state=1,n_jobs=2,verbose=1)
  forest.fit(X_train,y_train)
  res6 = forest.predict(X_test)
  print(metrics.classification_report(y_test.ravel(), res6.ravel(), digits=4))
  acc = sum(res6.ravel()==y_test.ravel())/len(y_test.ravel())
  ACCRF.append(acc)

  forest.fit(X_trainKF,y_trainKF)
  res6kf = forest.predict(X_testKF)
  print(metrics.classification_report(y_testKF.ravel(), res6kf.ravel(), digits=4))
  acckf = sum(res6kf.ravel()==y_testKF.ravel())/len(y_testKF.ravel())
  ACCRFKF.append(acckf)

print(np.shape(ACCRF))
plt.plot(ACCRF)
print(np.shape(ACCRFKF))
plt.plot(ACCRFKF)
plt.legend(['without kfold','after kfold'])
#plt.title('study on 0.3 KFold=2 Intensity test ACC v.s max_depth')
plt.title('Random Forest study on 0.3 KFold=2 ACC v.s neighbor numbers')

# Parameters
n_classes = N
plot_colors = "bry"
plot_step = 0.02
m = n
m = 2

#print([i for i in enumerate(np.unique(y))])

col = []
for i in range(n):
  for j in range(i+1,n):
    col.append([i, j])
    j += 1
print(col)

print(np.shape(X_train))
print(np.shape(y_train))

for pairidx, pair in enumerate(col):
#for pairidx, pair in enumerate([[0,1][1,2]]):
  print(np.array(pair,dtype = int))
  # We only take the two corresponding features
  X = X_train
  y = y_train
  print(np.shape(X))
  # Train
  clf = forest.fit(X, y)
  # Plot the decision boundary
  plt.plot()
  x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
  y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
#x_min, x_max = X.min() - 1, X.max() + 1
#y_min, y_max = y.min() - 1, y.max() + 1
#  x_min, x_max = X[:, pair].min() - 1, X[:, pair].max() + 1
#  y_min, y_max = X[:, pair].min() - 1, X[:, pair].max() + 1
  xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step), np.arange(y_min, y_max, plot_step))
  Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]) 
  Z = Z.reshape(xx.shape)
  cs = plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)
  plt.xlabel(str(pair[0]))
  plt.ylabel(str(pair[1]))
#plt.xlabel(str(0))
#plt.ylabel(str(1))
  plt.axis("tight")
  # Plot the training points（explit in 2D scatter）
  for i, color in zip(range(n_classes), plot_colors):
    idx = np.where(y == i)
    plt.scatter(X[idx, 0], X[idx, 1], c=color, label=str([i]),cmap=plt.cm.Paired)
    plt.axis("tight")
plt.suptitle("Decision surface of a decision tree using paired features")
plt.legend()
plt.show()

from sklearn.ensemble import AdaBoostClassifier
ACCABC = []
ACCABCKF = []
ada = AdaBoostClassifier()
ada.fit(X_train,y_train)
res7 = ada.predict(X_test)
print(res7)
print(metrics.classification_report(y_test.ravel(), res7.ravel(), digits=4))
acc = sum(res7.ravel()==y_test.ravel())/len(y_test.ravel())
ACCABC.append(acc)

ada.fit(X_trainKF,y_trainKF)
res7kf = ada.predict(X_testKF)
print(res7kf)
print(metrics.classification_report(y_testKF.ravel(), res7kf.ravel(), digits=4))
acckf = sum(res7kf.ravel()==y_testKF.ravel())/len(y_testKF.ravel())
ACCABCKF.append(acckf)

# Parameters
n_classes = N
plot_colors = "bry"
plot_step = 0.02
m = n
m = 2

#print([i for i in enumerate(np.unique(y))])

col = []
for i in range(n):
  for j in range(i+1,n):
    col.append([i, j])
    j += 1
print(col)

print(np.shape(X_train))
print(np.shape(y_train))

for pairidx, pair in enumerate(col):
#for pairidx, pair in enumerate([[0,1][1,2]]):
  print(np.array(pair,dtype = int))
  # We only take the two corresponding features
  X = X_train
  y = y_train
  print(np.shape(X))
  # Train
  clf = forest.fit(X, y)
  # Plot the decision boundary
  plt.plot()
  x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
  y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
#x_min, x_max = X.min() - 1, X.max() + 1
#y_min, y_max = y.min() - 1, y.max() + 1
#  x_min, x_max = X[:, pair].min() - 1, X[:, pair].max() + 1
#  y_min, y_max = X[:, pair].min() - 1, X[:, pair].max() + 1
  xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step), np.arange(y_min, y_max, plot_step))
  Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]) 
  Z = Z.reshape(xx.shape)
  cs = plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)
  plt.xlabel(str(pair[0]))
  plt.ylabel(str(pair[1]))
#plt.xlabel(str(0))
#plt.ylabel(str(1))
  plt.axis("tight")
  # Plot the training points（explit in 2D scatter）
  for i, color in zip(range(n_classes), plot_colors):
    idx = np.where(y == i)
    plt.scatter(X[idx, 0], X[idx, 1], c=color, label=str([i]),cmap=plt.cm.Paired)
    plt.axis("tight")
plt.suptitle("Decision surface of a decision tree using paired features")
plt.legend()
plt.show()

from matplotlib import get_backend
from sklearn.ensemble import GradientBoostingClassifier
ada.get_backend = GradientBoostingClassifier()
ACCGBC = []
ACCGBCKF = []

ada.fit(X_train,y_train)
res8 = ada.predict(X_test)
print(metrics.classification_report(y_test.ravel(), res8.ravel(), digits=4))
acc = sum(res8.ravel()==y_test.ravel())/len(y_test.ravel())
ACCGBC.append(acc)

ada.fit(X_trainKF,y_trainKF)
res8kf = ada.predict(X_testKF)
print(metrics.classification_report(y_testKF, res8kf, digits=4))
acckf = sum(res8.ravel()==y_testKF)/len(y_testKF)
ACCGBCKF.append(AccKF)

# Parameters
n_classes = N
plot_colors = "bry"
plot_step = 0.02
m = n
m = 2

#print([i for i in enumerate(np.unique(y))])

col = []
for i in range(n):
  for j in range(i+1,n):
    col.append([i, j])
    j += 1
print(col)

print(np.shape(X_train))
print(np.shape(y_train))

for pairidx, pair in enumerate(col):
#for pairidx, pair in enumerate([[0,1][1,2]]):
  print(np.array(pair,dtype = int))
  # We only take the two corresponding features
  X = X_train
  y = y_train
  print(np.shape(X))
  # Train
  clf = forest.fit(X, y)
  # Plot the decision boundary
  plt.plot()
  x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
  y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
#x_min, x_max = X.min() - 1, X.max() + 1
#y_min, y_max = y.min() - 1, y.max() + 1
#  x_min, x_max = X[:, pair].min() - 1, X[:, pair].max() + 1
#  y_min, y_max = X[:, pair].min() - 1, X[:, pair].max() + 1
  xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step), np.arange(y_min, y_max, plot_step))
  Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]) 
  Z = Z.reshape(xx.shape)
  cs = plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)
  plt.xlabel(str(pair[0]))
  plt.ylabel(str(pair[1]))
#plt.xlabel(str(0))
#plt.ylabel(str(1))
  plt.axis("tight")
  # Plot the training points（explit in 2D scatter）
  for i, color in zip(range(n_classes), plot_colors):
    idx = np.where(y == i)
    plt.scatter(X[idx, 0], X[idx, 1], c=color, label=str([i]),cmap=plt.cm.Paired)
    plt.axis("tight")
plt.suptitle("Decision surface of a decision tree using paired features")
plt.legend()
plt.show()

import matplotlib.pyplot as plt
from sklearn.neural_network import MLPClassifier
from sklearn.preprocessing import MinMaxScaler
from sklearn import datasets

# different learning rate schedules and momentum parameters
params = [{'solver': 'sgd', 'learning_rate': 'constant', 'momentum': 0,
           'learning_rate_init': 0.2},
          {'solver': 'sgd', 'learning_rate': 'constant', 'momentum': .9,
           'nesterovs_momentum': False, 'learning_rate_init': 0.2},
          {'solver': 'sgd', 'learning_rate': 'constant', 'momentum': .9,
           'nesterovs_momentum': True, 'learning_rate_init': 0.2},
          {'solver': 'sgd', 'learning_rate': 'invscaling', 'momentum': 0,
           'learning_rate_init': 0.2},
          {'solver': 'sgd', 'learning_rate': 'invscaling', 'momentum': .9,
           'nesterovs_momentum': True, 'learning_rate_init': 0.2},
          {'solver': 'sgd', 'learning_rate': 'invscaling', 'momentum': .9,
           'nesterovs_momentum': False, 'learning_rate_init': 0.2},
          {'solver': 'adam', 'learning_rate_init': 0.01}]
labels = ["constant learning-rate", "constant with momentum",
          "constant with Nesterov's momentum",
          "inv-scaling learning-rate", "inv-scaling with momentum",
          "inv-scaling with Nesterov's momentum", "adam"]
plot_args = [{'c': 'red', 'linestyle': '-'},
             {'c': 'green', 'linestyle': '-'},
             {'c': 'blue', 'linestyle': '-'},
             {'c': 'red', 'linestyle': '--'},
             {'c': 'green', 'linestyle': '--'},
             {'c': 'blue', 'linestyle': '--'},
             {'c': 'black', 'linestyle': '-'}]

def plot_on_dataset(X, y, ax, name):
    # for each dataset, plot learning for each learning strategy
    print("\nlearning on dataset %s" % name)
    ax.set_title(name)
    X = MinMaxScaler().fit_transform(X)
    mlps = []
    max_iter = 400
    for label, param in zip(labels, params):
        print("training: %s" % label)
        mlp = MLPClassifier(verbose=0, random_state=0,max_iter=max_iter, **param)
        mlp.fit(X, y)
        mlps.append(mlp)
        print("Training set score: %f" % mlp.score(X, y))
        print("Training set loss: %f" % mlp.loss_)
    for mlp, label, args in zip(mlps, labels, plot_args):
            ax.plot(mlp.loss_curve_, label=label, **args)

fig, axes = plt.subplots(1, 1, figsize=(15, 10))
#ax, data, name = zip(axes, (X, y), 'MLP Without Kfold') #*data = (X, y)
plot_on_dataset(X, y, ax=axes, name='MLP loss_curve Without Kfold')
#legend1 ax.get_lines()
fig.legend(ax.get_lines(), labels=labels, ncol=3, loc="upper center")
plt.show()

import numpy as np
from matplotlib import pyplot as plt
from matplotlib.colors import ListedColormap
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPClassifier

h = .02  # step size in the mesh
alphas = np.logspace(-5, 3, 5)
names = []
for i in alphas:
    names.append('alpha ' + str(i))
classifiers = []
for i in alphas:
    classifiers.append(MLPClassifier(alpha=i, random_state=1))
ncc = len(classifiers)
print(ncc)
rng = np.random.RandomState(2)
X += 2 * rng.uniform(size=X.shape)
linearly_separable = (X, y)

figure = plt.figure(figsize=(17, 9))

X = StandardScaler().fit_transform(X)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3)
x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5
y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h)) 
cm = plt.cm.RdBu
cm_bright = ListedColormap(['#FF0000', '#0000FF'])
ax = plt.figure()
# Plot the training points
plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)
# and testing points
plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6)
plt.xlim(xx.min(), xx.max())
plt.ylim(yy.min(), yy.max())
plt.xticks(())
plt.yticks(())
i = 1
# iterate over classifiers
for name, clf in zip(names, classifiers):
        ax = plt.subplot(2, 3, i)
        clf.fit(X_train[:, [0,1]], y_train)
        score = clf.score(X_test[:, [0,1]], y_test)
#        print(score)
#        print(np.shape(xx))
#        print(np.shape(yy))
#        print(np.shape(X_train))   
#        print(np.shape(y_test))      
        # Plot the decision boundary. For that, we will assign a color to each
        # point in the mesh [x_min, x_max]x[y_min, y_max].
        if hasattr(clf, "decision_function"):
          Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])
        else:
          Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]
        # Put the result into a color plot
        Z = Z.reshape(xx.shape)
        print(metrics.classification_report(y_test, clf.predict(X_test[:,[0,1]]), digits=4))
        ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)
        # Plot also the training points
        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)
        # and testing points
        ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,alpha=0.6)
        ax.set_xlim(xx.min(), xx.max())
        ax.set_ylim(yy.min(), yy.max())
        ax.set_xticks(())
        ax.set_yticks(())
        ax.set_title(name)
        ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),size=15, horizontalalignment='right')
        i += 1
figure.subplots_adjust(left=.02, right=.98)
plt.show()

